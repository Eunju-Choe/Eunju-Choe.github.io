---
title: FDM 실습 - 7장_의사결정나무 분석
author: Eunju Choe
category: Finance Data Mining
tags: [Lecture, Python, Data Mining, Finance]
img: ":20230321.png"
date: 2023-03-21 23:51:00 +0900
---
<p align="center"><img src="https://eunju-choe.github.io/assets/img/posts/20230321.png" width='50%'></p>

2021학년도 1학기 '금융데이터마이닝' 강의를 들을 때 작성했던 실습 파일입니다.
실습 수업을 들으며 작성한 코드이기 때문에 설명이 부실할 수 있습니다.

```python
import pandas as pd
import numpy as np

# 시각화
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.tree import plot_tree

# 전처리
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 알고리즘
import statsmodels.api as sm
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# 검정
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, \
recall_score, f1_score, roc_auc_score

# 최적화
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.model_selection import GridSearchCV
```

# Decision Tree 실습


```python
# 데이터 불러오기
df = pd.read_csv('cleaned_BankPersonalLoan.csv')

# X, Y 분리
X = df.drop('Personal Loan', axis = 1)
y = df['Personal Loan']

# train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, stratify = y)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

    (3500, 11) (1500, 11) (3500,) (1500,)



```python
# 모델 생성 및 학습
tree_m = DecisionTreeClassifier(criterion = 'gini', max_depth = None)
tree_m.fit(X_train, y_train)

# 테스트 데이터 예측 및 성과 평가
y_pred = tree_m.predict(X_test)
pd.DataFrame(confusion_matrix(y_test, y_pred))
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1349</td>
      <td>7</td>
    </tr>
    <tr>
      <th>1</th>
      <td>14</td>
      <td>130</td>
    </tr>
  </tbody>
</table>
</div>




```python
print('오분류 개수 : ', (y_test != y_pred).sum())
print('정확도 : ', round(accuracy_score(y_test, y_pred),3))
print('정밀도 : ', round(precision_score(y_test, y_pred),3))
print('재현율 : ', round(recall_score(y_test, y_pred),3))
print('f1 : ', round(f1_score(y_test, y_pred),3))
print('auc : ', round(roc_auc_score(y_test, y_pred),3))
```

    오분류 개수 :  21
    정확도 :  0.986
    정밀도 :  0.949
    재현율 :  0.903
    f1 :  0.925
    auc :  0.949


# GridSearchCV 실습


```python
pipe_tree = make_pipeline(DecisionTreeClassifier())
```


```python
# parameter 확인
pipe_tree.get_params().keys()
```




    dict_keys(['memory', 'steps', 'verbose', 'decisiontreeclassifier', 'decisiontreeclassifier__ccp_alpha', 'decisiontreeclassifier__class_weight', 'decisiontreeclassifier__criterion', 'decisiontreeclassifier__max_depth', 'decisiontreeclassifier__max_features', 'decisiontreeclassifier__max_leaf_nodes', 'decisiontreeclassifier__min_impurity_decrease', 'decisiontreeclassifier__min_impurity_split', 'decisiontreeclassifier__min_samples_leaf', 'decisiontreeclassifier__min_samples_split', 'decisiontreeclassifier__min_weight_fraction_leaf', 'decisiontreeclassifier__random_state', 'decisiontreeclassifier__splitter'])




```python
## GridSearchCV 최적화

param_crt = ['gini', 'entropy']
param_mxd = [None, 3, 4, 5, 6, 7, 8, 9, 10]
param_mss = [10, 20, 30, 40, 50]

param_grid = [{'decisiontreeclassifier__criterion' : param_crt,
               'decisiontreeclassifier__max_depth' : param_mxd,
               'decisiontreeclassifier__min_samples_split' : param_mss}]

clf_cv = GridSearchCV(estimator = pipe_tree,
            param_grid = param_grid,
            scoring = 'f1',
            n_jobs = -1,
            cv = 10)

clf_cv.fit(X_train, y_train)
```




    GridSearchCV(cv=10,
                 estimator=Pipeline(steps=[('decisiontreeclassifier',
                                            DecisionTreeClassifier())]),
                 n_jobs=-1,
                 param_grid=[{'decisiontreeclassifier__criterion': ['gini',
                                                                    'entropy'],
                              'decisiontreeclassifier__max_depth': [None, 3, 4, 5,
                                                                    6, 7, 8, 9,
                                                                    10],
                              'decisiontreeclassifier__min_samples_split': [10, 20,
                                                                            30, 40,
                                                                            50]}],
                 scoring='f1')




```python
print(round(clf_cv.best_score_, 3))
print(clf_cv.best_params_)
```

    0.916
    {'decisiontreeclassifier__criterion': 'entropy', 'decisiontreeclassifier__max_depth': 7, 'decisiontreeclassifier__min_samples_split': 10}



```python
tree_m_new = DecisionTreeClassifier(criterion = 'gini', max_depth = 6, min_samples_split = 10, random_state = 1)
tree_m_new.fit(X_train, y_train)
```




    DecisionTreeClassifier(max_depth=6, min_samples_split=10, random_state=1)



# Tree 시각화


```python
feature_name = X.columns.tolist()
target = np.array(['No', 'Yes'])

plt.figure(figsize = (18,15))

tree = plot_tree(tree_m_new, feature_names = feature_name,
                 class_names = target, filled = True,
                 rounded = True, fontsize = 12)
```


    
![png](https://eunju-choe.github.io/assets/img/posts/20230321-7/output_12_0.png)
    

